{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data and Select Subsample to Annotate (Users)\n",
    "\n",
    "This Python notebook describes the process of transforming the graph stored into a neo4j database into two files:\n",
    "\n",
    "- A .graphml with the users and their attributes\n",
    "- A .csv file the tweets and their respective users\n",
    "\n",
    "\n",
    "\n",
    "We begin by creating a file with the users and all their attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_user_graph.py\n",
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "nx_graph = nx.DiGraph()\n",
    "\n",
    "f = open(\"./twitter_neo4jsecret.json\", 'r')\n",
    "config_neo4j = json.load(f)\n",
    "f.close()\n",
    "graph = Graph(config_neo4j[\"host\"], password=config_neo4j[\"password\"])\n",
    "\n",
    "for node in graph.data(\"\"\"MATCH (a:User) WHERE a.virtual=\"F\" RETURN a as val\"\"\"):\n",
    "    n = dict(node[\"val\"])\n",
    "    nx_graph.add_node(n[\"id\"], **n)\n",
    "\n",
    "for node in graph.data(\n",
    "        \"\"\"MATCH (a:User)-->(b:User) WHERE a.virtual=\"F\" AND b.virtual=\"F\" RETURN a.id as a, b.id as b\"\"\"):\n",
    "    nx_graph.add_edge(node['a'], node['b'])\n",
    "\n",
    "nx.write_graphml(nx_graph, \"./users.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our initial graph will look like the following:\n",
    "\n",
    "![](./imgs/users_net1.png)\n",
    "\n",
    "Where each one of the nodes in the graph has a series of attributes such as:\n",
    "\n",
    "    id                      - unique identifier of a user     \n",
    "    created_at              - creation date of a profile in unix time\n",
    "    statuses_count          - number of statuses\n",
    "    lang                    - language inferred by twitter\n",
    "    listed_count            - number of lists that the user is at\n",
    "    number                  - number of user in data collection\n",
    "    favorites_count         - number of favorites the user has\n",
    "    uname                   - user name\n",
    "    verified                - whether the user is verified or not\n",
    "    default_profile         - whether the user has changed its profile from the default\n",
    "    default_profile_image   - whether the user has changed its profile image from the default\n",
    "    profile_image_url       - url to the user image\n",
    "    screen_name             - user screen name\n",
    "    geo_enabled             - whether the user's tweets are geo enabled\n",
    "    followees_count         - number of people the user follows\n",
    "    followers_count         - number of people following the user\n",
    "    description             - textual description of the user\n",
    "    location                - user's location\n",
    "    time_zone               - user's timezone, unstructured\n",
    "    \n",
    "    \n",
    "Interestingly, some characters are invalid for the graphml format, as there are emotijis and etc. Thus we have to remove invalid characters, which we do using the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unicode non-character U+1FFFF is not recommended for open interchange in print at -e line 1, <> line 375217.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "perl -CSDA -pe 's/[^\\x9\\xA\\xD\\x20-\\x{D7FF}\\x{E000}-\\x{FFFD}\\x{10000}-\\x{10FFFF}]+//g;' users.graphml > users2.graphml\n",
    "rm users.graphml\n",
    "mv users2.graphml users.graphml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the csv file with the texts created by the users and the following values:\n",
    "\n",
    "    screen_name       - screename of the user on twitter\n",
    "    tweet_id          - number with the unique identifier of a the tweet\n",
    "    tweet_text        - if tweet is a simple tweet or a quote, the text written\n",
    "    tweet_creation    - date in unix time when the tweet was tweeted\n",
    "    tweet_fav         - number of favorites\n",
    "    tweet_rt          - number of retweets\n",
    "    rp_flag           - flag that indicates if the tweet is a reply\n",
    "    rp_status         - id of the replied status\n",
    "    rp_user           - id of the replied user\n",
    "    qt_flag           - flag that indicates if the tweet is a quote\n",
    "    qt_user_id        - id of the quoted user\n",
    "    qt_status_id      - id of the quoted status\n",
    "    qt_text           - text of the quoted status\n",
    "    qt_creation       - date of creation of the quoted status\n",
    "    qt_fav            - number of favorites of the quoted status\n",
    "    qt_rt             - number of retweets of the quoted status\n",
    "    rt_flag           - flag that indicates if a tweet is a retweet\n",
    "    rt_user_id        - id of the retweeted user\n",
    "    rt_status_id      - id of the retweeted status\n",
    "    rt_text           - text of the retweeted status\n",
    "    rt_creation       - creation date of the retweeted status\n",
    "    rt_fav            - number of favorites of the retweeted status\n",
    "    rt_rt             - number of retweets of the retweeted status\n",
    "    \n",
    "The procedure to get that is quite complicated, mostly because the delimiter used in the crawler was ';', which is also legal character in twitter, thus we had to do some clever regex matching to get around this issue.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 386\n",
      "386 10386\n",
      "10386 20386\n",
      "20386 30386\n",
      "30386 40386\n",
      "40386 50386\n",
      "50386 60386\n",
      "60386 70386\n",
      "70386 80386\n",
      "80386 90386\n",
      "90386 100386\n"
     ]
    }
   ],
   "source": [
    "# get_tweets_table.py\n",
    "from py2neo import Graph\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "f = open(\"./twitter_neo4jsecret.json\", 'r')\n",
    "config_neo4j = json.load(f)\n",
    "f.close()\n",
    "graph = Graph(config_neo4j[\"host\"], password=config_neo4j[\"password\"])\n",
    "\n",
    "f = open(\"./tweets.csv\", \"w\")\n",
    "csv_writer = csv.writer(f)\n",
    "\n",
    "csv_writer.writerow([\"user_id\", \"screen_name\",\n",
    "                     \"tweet_id\", \"tweet_text\", \"tweet_creation\", \"tweet_fav\", \"tweet_rt\",\n",
    "                     \"rp_flag\", \"rp_status\", \"rp_user\",\n",
    "                     \"qt_flag\", \"qt_user_id\", \"qt_status_id\", \"qt_text\", \"qt_creation\", \"qt_fav\",  \"qt_rt\",\n",
    "                     \"rt_flag\", \"rt_user_id\", \"rt_status_id\", \"rt_text\", \"rt_creation\", \"rt_fav\", \"rt_rt\"])\n",
    "\n",
    "q = \"\"\"MATCH (u:User) where u.virtual=\"F\" return count(u) as number\"\"\"\n",
    "df = graph.data(q)\n",
    "max_entries = df[0][\"number\"]\n",
    "aux = (list(range(max_entries,0,-10000)) + [0])[::-1]\n",
    "ranges = zip(aux[:-1], aux[1:])\n",
    "\n",
    "for lower, upper in ranges:\n",
    "    print(lower, upper)\n",
    "\n",
    "    query = \"\"\" MATCH (u:User)-[:tweeted]->(t:Tweet)\n",
    "                WHERE u.number > {0} AND u.number < {1} \n",
    "                RETURN u.id as id, u.screen_name as screen_name, t.content as content \"\"\".format(lower, upper)\n",
    "\n",
    "    df = graph.data(query)\n",
    "\n",
    "    for row in df:\n",
    "        for tweet in row[\"content\"]:\n",
    "            new_tweet = []\n",
    "            len_tweet = len(tweet.split(\";\"))\n",
    "\n",
    "            match = re.match(\"([0-9])+\", tweet)\n",
    "            start, end = match.span()\n",
    "            new_tweet.append(tweet[start:end])\n",
    "\n",
    "            tweet = tweet[end + 1:]\n",
    "            match = re.match(\".*?(?=;1[0-9]{9}\\.0)\", tweet)\n",
    "            start, end = match.span()\n",
    "            new_tweet.append(tweet[start:end])\n",
    "            tweet = tweet[end + 1:]\n",
    "\n",
    "            tmp = tweet.split(\";\")\n",
    "            new_tweet += tmp[:9]\n",
    "            tweet = \";\".join(tmp[9:])\n",
    "\n",
    "            if new_tweet[-3] == 'True':\n",
    "                match = re.match(\".*?(?=;1[0-9]{9}\\.0)\", tweet)\n",
    "                start, end = match.span()\n",
    "                new_tweet.append(tweet[start:end])\n",
    "                tweet = tweet[end + 1:]\n",
    "                tmp = tweet.split(\";\")\n",
    "                new_tweet += tmp[:6]\n",
    "                tweet = \";\".join(tmp[6:])\n",
    "\n",
    "            else:\n",
    "                tmp = tweet.split(\";\")\n",
    "                new_tweet += tmp[:7]\n",
    "                tweet = \";\".join(tmp[7:])\n",
    "\n",
    "            match = re.match(\".*?(?=;1[0-9]{9}\\.0)\", tweet)\n",
    "            if match:\n",
    "                start, end = match.span()\n",
    "                new_tweet.append(tweet[start:end])\n",
    "                tweet = tweet[end + 1:]\n",
    "            tmp = tweet.split(\";\")\n",
    "            new_tweet += tmp\n",
    "\n",
    "            csv_writer.writerow([row[\"id\"]] + [row[\"screen_name\"]] + new_tweet)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ultimate goal is to find hateful users in the network we extracted from twitter. We will then select a subsample of the users to be annotated as hateful or not hateful. However this is not trivial as hateful users probably are a minority on Twitter, making undesirable to simply take a random sample (as annotating the users is expensive). Our technique for selecting the subsampling, thus, follows the following broad strokes:\n",
    "\n",
    "1. Select a subset of users which are likely to be hateful.\n",
    "2. Find users that are close to these users in the network.\n",
    "3. Select the users to annotate from the users which are hateful and the ones which are close to them in the network.\n",
    "\n",
    "This method follows the principle of homophily, which is commonly used in the Social Network Analysis literature. We now explain in some detail how we implement the steps $1$, $2$ and $3$.\n",
    "\n",
    "## Selecting users that are likely to be hateful\n",
    "\n",
    "We will mark as *infected* users that used a word in a lexicon of words highly correlated with hateful speech. Some examples are:\n",
    "\n",
    "    goy, kike, heeb, nigga, white genocide, anti white, racial realism, holohoax, racemixing, racial treason\n",
    "    \n",
    "We will find the users who done it in the texts we have just extracted, and then mark them as infected in the graph we generated! Notice that in our previous graph a directed edge ($u_1$,$u_2$) meant that user $u_1$ retweeted $u_2$. However, influence in the retweet network is interpreted the other way around, if $u_1$ retweeted $u_2$, it actually means that $u_2$ influences $u_1$. This is clearly not always the case, as users may retweet other users sarcastically, but is a good model, commonly used in the Social Network Analysis literature.\n",
    "\n",
    "![](./imgs/users_net2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import re\n",
    "\n",
    "l = open(\"./lexicon.txt\", \"r\")\n",
    "regexp = \"\"\n",
    "for line in l.readlines():\n",
    "    regexp += \"({0})|\".format(line.rstrip())\n",
    "l.close()\n",
    "regexp = regexp[:-1]\n",
    "regexp = re.compile(regexp)\n",
    "\n",
    "f = open(\"./tweets.csv\", \"r\")\n",
    "re.match(regexp, \"\")\n",
    "csv_writer = csv.DictReader(f)\n",
    "\n",
    "set_users = dict()\n",
    "\n",
    "for line in csv_writer:\n",
    "    text = regexp.search(line[\"tweet_text\"])\n",
    "    retweet = regexp.search(line[\"rt_text\"])\n",
    "    quote = regexp.search(line[\"qt_text\"])\n",
    "    if text is not None or retweet is not None or quote is not None:\n",
    "        set_users[line[\"user_id\"]] = True\n",
    "\n",
    "nx_graph = nx.read_graphml(\"./users.graphml\")\n",
    "nx_graph = nx_graph.reverse(copy=False)\n",
    "nx.set_node_attributes(nx_graph, name=\"slur\", values=set_users)\n",
    "nx.write_graphml(nx_graph, \"./users_infected.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some users who are suspicious to be hateful users. However, the problem is that we are limiting ourselves to the users who used one of our words. This will be addressed in the next step of our modelling. However, first, let us visualize the network \"as is\", with the infected users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abusive_users_osn",
   "language": "python",
   "name": "abusive_users_osn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
