{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Database to Annotation Ready\n",
    "\n",
    "\n",
    "This Python notebook describes the process of transforming the graph stored into a neo4j database into three files:\n",
    "\n",
    "- `users_infected_diffusion.graphml` with the users their attributes, and the diffusion;\n",
    "\n",
    "- `tweets.csv` with the tweets and their respective users;\n",
    "\n",
    "- `users_to_annotate.csv` a csv file with the 5071 users to be annotated.\n",
    "\n",
    "We begin by creating a `.graphml` file with the users and all their attributes:\n",
    "\n",
    "`database` $\\rightarrow$ `users.graphml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "nx_graph = nx.DiGraph()\n",
    "\n",
    "f = open(\"../secrets/twitter_neo4jsecret.json\", 'r')\n",
    "config_neo4j = json.load(f)\n",
    "f.close()\n",
    "graph = Graph(config_neo4j[\"host\"], password=config_neo4j[\"password\"])\n",
    "\n",
    "for node in graph.data(\"\"\"MATCH (a:User) WHERE a.virtual=\"F\" RETURN a as val\"\"\"):\n",
    "    n = dict(node[\"val\"])\n",
    "    nx_graph.add_node(n[\"id\"], **n)\n",
    "    nx_graph.add_edge(n[\"id\"], n[\"id\"])\n",
    "\n",
    "for node in graph.data(\n",
    "        \"\"\"MATCH (a:User)-[:retweeted]->(b:User) WHERE a.virtual=\"F\" AND b.virtual=\"F\" RETURN a.id as a, b.id as b\"\"\"):\n",
    "    nx_graph.add_edge(node['a'], node['b'])\n",
    "\n",
    "nx.write_graphml(nx_graph, \"../data/users.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our initial graph will look like the following:\n",
    "\n",
    "![](../imgs/users_net1.png)\n",
    "\n",
    "Where each one of the nodes in the graph has the following attributes:\n",
    "\n",
    "    id                      - unique identifier of a user     \n",
    "    statuses_count          - number of statuses\n",
    "    lang                    - language inferred by twitter\n",
    "    listed_count            - number of lists that the user is at\n",
    "    number                  - number of user in data collection\n",
    "    favorites_count         - number of favorites the user has\n",
    "    uname                   - user name\n",
    "    verified                - the user is verified or not\n",
    "    default_profile         - the user has changed the default profile\n",
    "    default_profile_image   - the user has changed the default image\n",
    "    profile_image_url       - url to the user image\n",
    "    screen_name             - user screen name\n",
    "    geo_enabled             - whether the user's tweets are geo enabled\n",
    "    followees_count         - number of people the user follows\n",
    "    followers_count         - number of people following the user\n",
    "    description             - textual description of the user\n",
    "    location                - user's location\n",
    "    time_zone               - user's timezone, unstructured\n",
    "    \n",
    "    \n",
    "Interestingly, some characters are invalid for the graphml format, as there are emotijis and etc. Thus we have to remove invalid characters, which we do using the following script.\n",
    "\n",
    "`users.graphml` $\\rightarrow$ `users.graphml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "perl -CSDA -pe 's/[^\\x9\\xA\\xD\\x20-\\x{D7FF}\\x{E000}-\\x{FFFD}\\x{10000}-\\x{10FFFF}]+//g;' ../data/users.graphml > ../data/users2.graphml\n",
    "rm ../data/users.graphml\n",
    "mv ../data/users2.graphml ../data/users.graphml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the csv file with the texts created by the users and the following values:\n",
    "\n",
    "    screen_name       - screename of the user on twitter\n",
    "    tweet_id          - number with the identifier of a the tweet\n",
    "    tweet_text        - if status is a tweet/quote, the text written\n",
    "    tweet_creation    - date in unix time when the tweet was tweeted\n",
    "    tweet_fav         - number of favorites\n",
    "    tweet_rt          - number of retweets\n",
    "    rp_flag           - flag that indicates if the tweet is a reply\n",
    "    rp_status         - id of the replied status\n",
    "    rp_user           - id of the replied user\n",
    "    qt_flag           - flag that indicates if the tweet is a quote\n",
    "    qt_user_id        - id of the quoted user\n",
    "    qt_status_id      - id of the quoted status\n",
    "    qt_text           - text of the quoted status\n",
    "    qt_creation       - date of creation of the quoted status\n",
    "    qt_fav            - number of favorites of the quoted status\n",
    "    qt_rt             - number of retweets of the quoted status\n",
    "    rt_flag           - flag that indicates if a tweet is a retweet\n",
    "    rt_user_id        - id of the retweeted user\n",
    "    rt_status_id      - id of the retweeted status\n",
    "    rt_text           - text of the retweeted status\n",
    "    rt_creation       - creation date of the retweeted status\n",
    "    rt_fav            - number of favorites of the retweeted status\n",
    "    rt_rt             - number of retweets of the retweeted status\n",
    "    \n",
    "The procedure to get that is quite complicated, mostly because the delimiter used in the crawler was ';', which is also legal character in twitter, thus we had to do some clever regex matching to get around this issue.\n",
    "\n",
    "`database` $\\rightarrow$ `tweets.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "f = open(\"../secrets/twitter_neo4jsecret.json\", 'r')\n",
    "config_neo4j = json.load(f)\n",
    "f.close()\n",
    "graph = Graph(config_neo4j[\"host\"], password=config_neo4j[\"password\"])\n",
    "\n",
    "f = open(\"../data/tweets.csv\", \"w\")\n",
    "csv_writer = csv.writer(f)\n",
    "\n",
    "csv_writer.writerow([\"user_id\", \"screen_name\",\n",
    "                     \"tweet_id\", \"tweet_text\", \"tweet_creation\", \"tweet_fav\", \"tweet_rt\",\n",
    "                     \"rp_flag\", \"rp_status\", \"rp_user\",\n",
    "                     \"qt_flag\", \"qt_user_id\", \"qt_status_id\", \"qt_text\", \"qt_creation\", \"qt_fav\", \"qt_rt\",\n",
    "                     \"rt_flag\", \"rt_user_id\", \"rt_status_id\", \"rt_text\", \"rt_creation\", \"rt_fav\", \"rt_rt\"])\n",
    "\n",
    "q = \"\"\"MATCH (u:User) where u.virtual=\"F\" return count(u) as number\"\"\"\n",
    "df = graph.data(q)\n",
    "max_entries = df[0][\"number\"]\n",
    "aux = (list(range(max_entries, 0, -10000)) + [0])[::-1]\n",
    "ranges = zip(aux[:-1], aux[1:])\n",
    "\n",
    "for lower, upper in ranges:\n",
    "    print(lower, upper)\n",
    "\n",
    "    query = \"\"\" MATCH (u:User)-[:tweeted]->(t:Tweet)\n",
    "                WHERE u.number > {0} AND u.number < {1} \n",
    "                RETURN u.id as id, u.screen_name as screen_name, t.content as content \"\"\".format(lower, upper)\n",
    "\n",
    "    df = graph.data(query)\n",
    "\n",
    "    for row in df:\n",
    "        for tweet in row[\"content\"]:\n",
    "            new_tweet = []\n",
    "            len_tweet = len(tweet.split(\";\"))\n",
    "\n",
    "            match = re.match(\"([0-9])+\", tweet)\n",
    "            start, end = match.span()\n",
    "            new_tweet.append(tweet[start:end])\n",
    "\n",
    "            tweet = tweet[end + 1:]\n",
    "            match = re.match(\".*?(?=;1[0-9]{9}\\.0)\", tweet)\n",
    "            start, end = match.span()\n",
    "            new_tweet.append(tweet[start:end])\n",
    "            tweet = tweet[end + 1:]\n",
    "\n",
    "            tmp = tweet.split(\";\")\n",
    "            new_tweet += tmp[:9]\n",
    "            tweet = \";\".join(tmp[9:])\n",
    "\n",
    "\n",
    "            if new_tweet[-3] == 'True':\n",
    "                match = re.match(\".*?(?=;1[0-9]{9}\\.0)\", tweet)\n",
    "                start, end = match.span()\n",
    "                new_tweet.append(tweet[start:end])\n",
    "                tweet = tweet[end + 1:]\n",
    "                tmp = tweet.split(\";\")\n",
    "                new_tweet += tmp[:6]\n",
    "                tweet = \";\".join(tmp[6:])\n",
    "\n",
    "            else:\n",
    "                tmp = tweet.split(\";\")\n",
    "                new_tweet += tmp[:7]\n",
    "                tweet = \";\".join(tmp[7:])\n",
    "\n",
    "            match = re.match(\".*?(?=;1[0-9]{9}\\.0)\", tweet)\n",
    "            if match:\n",
    "                start, end = match.span()\n",
    "                new_tweet.append(tweet[start:end])\n",
    "                tweet = tweet[end + 1:]\n",
    "            tmp = tweet.split(\";\")\n",
    "            new_tweet += tmp\n",
    "\n",
    "            csv_writer.writerow([row[\"id\"]] + [row[\"screen_name\"]] + new_tweet)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ultimate goal is to find hateful users in the network we extracted from twitter. We will then select a subsample of the users to be annotated as hateful or not hateful. However this is not trivial as hateful users probably are a minority on Twitter, making undesirable to simply take a random sample (as annotating the users is expensive). Our technique for selecting the subsampling, thus, follows the following broad strokes:\n",
    "\n",
    "1. Select a subset of users which are likely to be hateful.\n",
    "2. Find users that are close to these users in the network.\n",
    "3. Select the users to annotate from the users which are hateful and the ones which are close to them in the network.\n",
    "\n",
    "This method follows the principle of homophily, which is commonly used in the Social Network Analysis literature. We now explain in some detail how we implement the steps $1$, $2$ and $3$.\n",
    "\n",
    "## 1. Select a subset of users which are likely to be hateful.\n",
    "\n",
    "We will mark as *infected* users that used a word in a lexicon of words highly correlated with hateful speech. Some examples are:\n",
    "\n",
    "    goy, kike, heeb, nigga, white genocide, anti white, racial realism, holohoax, racemixing, racial treason\n",
    "    \n",
    "We will find the users who done it in the texts we have just extracted, and then mark them as infected in the graph we generated! Notice that in our previous graph a directed edge ($u_1$,$u_2$) meant that user $u_1$ retweeted $u_2$. However, influence in the retweet network is interpreted the other way around, if $u_1$ retweeted $u_2$, it actually means that $u_2$ influences $u_1$. This is clearly not always the case, as users may retweet other users sarcastically, but is a good model, commonly used in the Social Network Analysis literature.\n",
    "\n",
    "![](../imgs/users_net2.png)\n",
    "\n",
    "`users.graphml` `tweets.csv` `lexicon.txt` $\\rightarrow$ `users_infected.graphml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import re\n",
    "\n",
    "l = open(\"../data/lexicon.txt\", \"r\")\n",
    "regexp = \"\"\n",
    "for line in l.readlines():\n",
    "    regexp += \"({0})|\".format(line.rstrip())\n",
    "l.close()\n",
    "regexp = regexp[:-1]\n",
    "regexp = re.compile(regexp)\n",
    "\n",
    "f = open(\"../data/tweets.csv\", \"r\")\n",
    "re.match(regexp, \"\")\n",
    "csv_writer = csv.DictReader(f)\n",
    "\n",
    "set_users = dict()\n",
    "\n",
    "for line in csv_writer:\n",
    "    text = regexp.search(line[\"tweet_text\"])\n",
    "    retweet = regexp.search(line[\"rt_text\"])\n",
    "    quote = regexp.search(line[\"qt_text\"])\n",
    "    if text is not None or retweet is not None or quote is not None:\n",
    "        set_users[line[\"user_id\"]] = True\n",
    "f.close()\n",
    "\n",
    "\n",
    "nx_graph = nx.read_graphml(\"../data/users.graphml\")\n",
    "nx_graph = nx_graph.reverse(copy=False)\n",
    "nx.set_node_attributes(nx_graph, name=\"slur\", values=set_users)\n",
    "nx.write_graphml(nx_graph, \"../data/users_infected.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some users who are suspicious to be hateful users. However, the problem is that we are limiting ourselves to the users who used one of our words. This will be addressed in the next step of our modelling. However, first, let us visualize the network \"as is\", with the infected users:\n",
    "\n",
    "<img src=\"../imgs/users_diffusion.png\" alt=\"\" style=\"width: 550px;\"/>\n",
    "\n",
    "\n",
    "## 2. Find users that are close to these users in the network.\n",
    "\n",
    "To get a more general sample, and not very biased towards our lexicon, we employ a diffusion proccess on the graph considering the users that employed the lexicon as \"infected\". Our model is based on DeGroot learning model, as described in Golub & Jackson (2010). Given the matrix $G$ of the retweet induced graph:\n",
    "\n",
    "- Let $T$ is $G$ normalized in a row matrix. In other words, every row sums to $1$. In practice, this means that every user $u$ is influenced equally by all its neighbors in the graph. \n",
    "- Let $p$ be an vector representing the infected users. $p^{(i)} = 1$ if the $i$-th user was marked as infected and $p^{(i)} = 0$ otherwise.\n",
    "\n",
    "Then we perform the following diffusion process, for $k$ steps, do:\n",
    "\n",
    "$$\n",
    "p^{(k)} = T*p^{(k-1)}\n",
    "$$\n",
    "\n",
    "`users_infected.graphml` $\\rightarrow$ `users_infected_diffusion.graphml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "initial_belief = 1\n",
    "k = 2\n",
    "\n",
    "np.random.seed(1)\n",
    "graph = nx.read_graphml(\"../data/users_infected.graphml\")\n",
    "\n",
    "slur_nodes = list(nx.get_node_attributes(graph, \"slur\"))\n",
    "other_nodes = list(set(graph.nodes()).difference(set(slur_nodes)))\n",
    "node_list = slur_nodes + other_nodes\n",
    "\n",
    "transition_matrix = nx.adjacency_matrix(graph, nodelist=node_list).asfptype()\n",
    "n = transition_matrix.shape[0]\n",
    "\n",
    "for i in range(n):\n",
    "    total = transition_matrix[i, :].sum()\n",
    "    if total != 0:\n",
    "        transition_matrix[i, :] = transition_matrix[i, :] / total\n",
    "\n",
    "\n",
    "beliefs = np.zeros(len(node_list))\n",
    "beliefs[:len(slur_nodes)] = initial_belief\n",
    "\n",
    "for _ in range(k):\n",
    "    out = transition_matrix.dot(beliefs)\n",
    "    beliefs = out\n",
    "\n",
    "\n",
    "final_beliefs_dict = dict()\n",
    "for node, belief in zip(node_list, beliefs):\n",
    "    final_beliefs_dict[node] = float(belief)\n",
    "\n",
    "nx.set_node_attributes(graph, name=\"diffusion_slur\", values=final_beliefs_dict)\n",
    "nx.write_graphml(graph, \"../data/users_infected_diffusion.graphml\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $k=2$, we have then assigned values from $[0,1]$ which show how close our users are to those that used the words in the lexicon. We have something like in:\n",
    "![](../imgs/users_net3.png)\n",
    "\n",
    "We can also visualize the real graph:\n",
    "\n",
    "<img src=\"../imgs/users.png\" alt=\"\" style=\"width: 550px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select the users to annotate from the users which are hateful and the ones which are close to them in the network.\n",
    "\n",
    "Finally Divide the users in $4$ strata according to their associated beliefs after the diffusion process, and perform a stratified sampling, obtaining up to $1500$ user per strata. Notice that $p_{i}^{(t)} \\in [0,1]$. With this real value associated with each user, we get 4 strata by randomly selecting up to $1500$ users with $p_{i}$ in the intervals $[0,.25)$, $[.25,.50)$, $[.50,.75)$ and $[.75,1]$.\n",
    "\n",
    "`users_infected_diffusion.graphml` $\\rightarrow$ `users_to_annotate.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "N = 6000\n",
    "\n",
    "nx_graph = nx.read_graphml(\"../data/users_infected_diffusion.graphml\")\n",
    "diffusion_slur = nx.get_node_attributes(nx_graph, name=\"diffusion_slur\")\n",
    "screen_names = nx.get_node_attributes(nx_graph, name=\"screen_name\")\n",
    "in_degree = nx_graph.in_degree()\n",
    "strata1, strata2, strata3, strata4 = [], [], [], []\n",
    "sum_vals = 0\n",
    "\n",
    "for key in sorted(diffusion_slur):\n",
    "\n",
    "    if diffusion_slur[key] < .25:\n",
    "        strata1.append(int(key))\n",
    "\n",
    "    if .50 > diffusion_slur[key] >= .25:\n",
    "        strata2.append(int(key))\n",
    "\n",
    "    if .75 > diffusion_slur[key] >= .50:\n",
    "        strata3.append(int(key))\n",
    "\n",
    "    if diffusion_slur[key] >= .75:\n",
    "        strata4.append(int(key))\n",
    "\n",
    "sample_strata1 = np.random.choice(strata1, size=int(N / 4), replace=False)\n",
    "sample_strata2 = np.random.choice(strata2, size=int(N / 4), replace=False)\n",
    "sample_strata3 = np.random.choice(strata3, size=int(N / 4), replace=False)\n",
    "sample_strata4 = np.random.choice(strata4, size=min(int(N / 4), len(strata4)), replace=False)\n",
    "\n",
    "f = open(\"../data/users_to_annotate.csv\", \"w\")\n",
    "\n",
    "csv_writer = csv.writer(f)\n",
    "\n",
    "csv_writer.writerow([\"user_id\", \"screen_name\", \"twitter\", \"diffusion_slur\", \"stratum\"])\n",
    "\n",
    "count = 0\n",
    "\n",
    "sample = []\n",
    "\n",
    "for strata in [sample_strata1, sample_strata2, sample_strata3, sample_strata4]:\n",
    "    count += 1\n",
    "    for key in strata:\n",
    "        sample.append([int(key),\n",
    "                       screen_names[str(key)],\n",
    "                       \"https://twitter.com/{0}\".format(screen_names[str(key)]),\n",
    "                       diffusion_slur[str(key)], count])\n",
    "\n",
    "print(sample)\n",
    "random.shuffle(sample)\n",
    "\n",
    "for row in sample:\n",
    "    csv_writer.writerow(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abusive_users_osn",
   "language": "python",
   "name": "abusive_users_osn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
