{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Annotations to Features\n",
    "\n",
    "This Python notebook describes the process of the transforming:\n",
    "\n",
    "- `users_infected_diffusion.graphml` with the users their attributes, and the diffusion;\n",
    "\n",
    "- `tweets.csv` with the tweets and their respective users;\n",
    "\n",
    "- `users_to_annotate.csv` a csv file with the 5071 users to be annotated.\n",
    "\n",
    "- `annotated_full.csv` a csv file with the results in the annotation.\n",
    "\n",
    "- `created_at.csv` a csv file with the creation date for the annotated users. This was collected after the main data collection, due to a bug in the data collection script (which has been fixed).\n",
    "\n",
    "- `deleted_account.csv` a csv file with whether the accounts have been deleted after 3 months.\n",
    "\n",
    "Into the following files:\n",
    "\n",
    "- `users_all_neigh.csv` a csv file with the features extracted for the $100386$ users.\n",
    "\n",
    "- `users_all_neigh_anon.csv` an anonymous version of the previous file.\n",
    "\n",
    "- A set of files to be used by GraphSage:\n",
    "\n",
    "    - `sw-G.json` -- A networkx-specified json file describing the input graph. Nodes have 'val' and 'test' attributes specifying if they are a part of the validation and test sets, respectively.\n",
    "    - `sw-id_map.json` -- A json-stored dictionary mapping the graph node ids to consecutive integers.\n",
    "    - `sw-class_map.json` -- A json-stored dictionary mapping the graph node ids to classes.\n",
    "    - `sw-feats.npy` --- A numpy-stored array of node features; ordering given by id_map.json. Can be omitted and only identity features will be used.\n",
    "    \n",
    "## Annotated_graph\n",
    "    \n",
    "We begin using `annotated_full.csv`and `users_infected_diffusion.graphml` to create a new graph which has annotations, and also centrality measures (why not do the two things at the same time right?)!\n",
    "\n",
    "`annotated_full.csv` `users_infected_diffusion.graphml` $\\rightarrow$ `users_hate.graphml`\n",
    "\n",
    "`1_average_intervals.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Read annotated users\n",
    "\n",
    "f = open(\"../data/annotated_full.csv\", \"r\")\n",
    "csv_writer = csv.DictReader(f)\n",
    "\n",
    "set_users = dict()\n",
    "c = 0\n",
    "tmp = set()\n",
    "for line in csv_writer:\n",
    "    if line[\"hate\"] == '1':\n",
    "        tmp.add(line[\"user_id\"])\n",
    "        set_users[line[\"user_id\"]] = 1\n",
    "    elif line[\"hate\"] == \"0\":\n",
    "        set_users[line[\"user_id\"]] = 0\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Set hate attributes\n",
    "\n",
    "nx_graph = nx.read_graphml(\"../data/users_infected_diffusion.graphml\")\n",
    "\n",
    "\n",
    "nx.set_node_attributes(nx_graph, name=\"hate\", values=-1)\n",
    "nx.set_node_attributes(nx_graph, name=\"hate\", values=set_users)\n",
    "# Set hateful and normal neighbors attribute\n",
    "\n",
    "nodes = nx_graph.nodes(data='hate')\n",
    "\n",
    "hateful_neighbors = dict()\n",
    "normal_neighbors = dict()\n",
    "\n",
    "for i in nodes:\n",
    "    if i[1] == 1:  # hateful node\n",
    "        for j in nx_graph.neighbors(i[0]):\n",
    "            hateful_neighbors[j] = True\n",
    "    if i[1] == 0:\n",
    "        for j in nx_graph.neighbors(i[0]):\n",
    "            normal_neighbors[j] = True\n",
    "\n",
    "nx.set_node_attributes(nx_graph, name=\"hateful_neighbors\", values=False)\n",
    "nx.set_node_attributes(nx_graph, name=\"hateful_neighbors\", values=hateful_neighbors)\n",
    "nx.set_node_attributes(nx_graph, name=\"normal_neighbors\", values=False)\n",
    "nx.set_node_attributes(nx_graph, name=\"normal_neighbors\", values=normal_neighbors)\n",
    "\n",
    "\n",
    "# Set node network-based attributes, such as betweenness and eigenvector\n",
    "\n",
    "vt = time.time()\n",
    "betweenness = nx.betweenness_centrality(nx_graph, k=16258, normalized=False)\n",
    "eigenvector = nx.eigenvector_centrality(nx_graph)\n",
    "in_degree = nx.in_degree_centrality(nx_graph)\n",
    "out_degree = nx.out_degree_centrality(nx_graph)\n",
    "\n",
    "nx.set_node_attributes(nx_graph, name=\"betweenness\", values=betweenness)\n",
    "nx.set_node_attributes(nx_graph, name=\"eigenvector\", values=eigenvector)\n",
    "nx.set_node_attributes(nx_graph, name=\"in_degree\", values=in_degree)\n",
    "nx.set_node_attributes(nx_graph, name=\"out_degree\", values=out_degree)\n",
    "\n",
    "nx.write_graphml(nx_graph, \"../data/users_hate.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a csv file with users and these attributes:\n",
    "\n",
    "    user_id            - unique identifier of a user \n",
    "    hate               - hateful|normal|other\n",
    "    hate_neigh         - True|False\n",
    "    normal_neigh       - True|False\n",
    "    statuses_count     - number of statuses\n",
    "    followers_count    - number of followers\n",
    "    followees_count    - number of followees\n",
    "    favorites_count    - number of favorites\n",
    "    listed_count       - number of listed\n",
    "    median_int         - median interval between tweets\n",
    "    average_int        - average interval between tweets\n",
    "    betweenness        - centrality measure\n",
    "    eigenvector        - centrality measure\n",
    "    in_degree          - centrality measure\n",
    "    out_degree         - centrality measure\n",
    "    \n",
    "`users_hate.graphml` $\\rightarrow$ `users_attributes.csv`\n",
    "\n",
    "`2_get_user_attributes.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "nx_graph = nx.read_graphml(\"../data/users_hate.graphml\")\n",
    "\n",
    "hate = nx.get_node_attributes(nx_graph, \"hate\")\n",
    "\n",
    "hate_n = nx.get_node_attributes(nx_graph, \"hateful_neighbors\")\n",
    "normal_n = nx.get_node_attributes(nx_graph, \"normal_neighbors\")\n",
    "betweenness = nx.get_node_attributes(nx_graph, \"betweenness\")\n",
    "eigenvector = nx.get_node_attributes(nx_graph, \"eigenvector\")\n",
    "in_degree = nx.get_node_attributes(nx_graph, \"in_degree\")\n",
    "out_degree = nx.get_node_attributes(nx_graph, \"out_degree\")\n",
    "statuses_count = nx.get_node_attributes(nx_graph, \"statuses_count\")\n",
    "followers_count = nx.get_node_attributes(nx_graph, \"followers_count\")\n",
    "followees_count = nx.get_node_attributes(nx_graph, \"followees_count\")\n",
    "favorites_count = nx.get_node_attributes(nx_graph, \"favorites_count\")\n",
    "listed_count = nx.get_node_attributes(nx_graph, \"listed_count\")\n",
    "\n",
    "users = []\n",
    "\n",
    "for user_id in hate.keys():\n",
    "    hateful = \"other\"\n",
    "\n",
    "    if hate[user_id] == 1:\n",
    "        hateful = \"hateful\"\n",
    "\n",
    "    elif hate[user_id] == 0:\n",
    "        hateful = \"normal\"\n",
    "\n",
    "    users.append((user_id, hateful, hate_n[user_id], normal_n[user_id],  # General Stuff\n",
    "                  statuses_count[user_id], followers_count[user_id], followees_count[user_id],\n",
    "                  favorites_count[user_id], listed_count[user_id],  # Numeric attributes\n",
    "                  betweenness[user_id], eigenvector[user_id],  # Network Attributes\n",
    "                  in_degree[user_id], out_degree[user_id]))\n",
    "\n",
    "columns = [\"user_id\", \"hate\", \"hate_neigh\", \"normal_neigh\", \"statuses_count\", \"followers_count\", \"followees_count\",\n",
    "           \"favorites_count\", \"listed_count\", \"betweenness\", \"eigenvector\", \"in_degree\", \"out_degree\"]\n",
    "\n",
    "df = pd.DataFrame.from_records(users, columns=columns)\n",
    "\n",
    "df.to_csv(\"../data/users_attributes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Content-based attributes\n",
    "\n",
    "We now start dealing with the content-based attributes. Notice that the next two scripts actually calculate We begin by calculating \"simple\" attributes based on content. We begin extracting the following from the tweets of each user:\n",
    "\n",
    "    number hashtags          - number of hashtags used by the user\n",
    "    hashtags                 - string with the hashtags\n",
    "    tweet number             - number of tweets (original)\n",
    "    retweet number           - number of retweets\n",
    "    quote number             - number of quotes\n",
    "    status length            - length of the tweets (average)\n",
    "    number urls              - number of urls in the tweets\n",
    "    baddies                  - number of bad words\n",
    "    mentions                 - number of mentions\n",
    "    \n",
    "`tweets.csv` `bad_words.txt` $\\rightarrow$ `./tmp2/users_content_*`\n",
    "\n",
    "`3_content_attributes1.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 ./5_content_attributes1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we procced to extract more complicated features from text, most specifically, we create vector representations based on Empath and on GloVe. We also perform sentiment analysis using VADER.\n",
    "\n",
    "    *_empath hashtags        - 100+ empath categories\n",
    "    *_glove                  - 300 dim glove vector\n",
    "    sentiment                - sentiment score\n",
    "    subjectivity             - subjectivity score\n",
    "\n",
    "\n",
    "`tweets.csv` $\\rightarrow$ `./tmp/users_content_*`\n",
    "\n",
    "`4_content_attributes2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 ./6_content_attributes2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a bash script to put together these files (which were separated in the first place so we could parallelize the proccess);\n",
    "\n",
    "`tmp/*.csv` $\\rightarrow$ `users_content.csv`\n",
    "\n",
    "`tmp2/*.csv` $\\rightarrow$ `users_content2.csv`\n",
    "\n",
    "`7_concat_files.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OutFileName=\"../data/users_content.csv\"            # Fix the output name\n",
    "i=0                                                # Reset a counter\n",
    "for filename in ../data/tmp/*.csv; do\n",
    "    if [ \"$filename\"  != \"$OutFileName\" ] ;        # Avoid recursion\n",
    "    then\n",
    "    if [[ $i -eq 0 ]] ; then\n",
    "       head -1  $filename >   $OutFileName         # Copy header if it is the first file\n",
    "    fi\n",
    "    tail -n +2  $filename >>  $OutFileName         # Append from the 2nd line each file\n",
    "    i=$(( $i + 1 ))                                # Increase the counter\n",
    "    fi\n",
    "done\n",
    "OutFileName=\"../data/users_content2.csv\"           # Fix the output name\n",
    "i=0                                                # Reset a counter\n",
    "for filename in ../data/tmp2/*.csv; do\n",
    "    if [ \"$filename\"  != \"$OutFileName\" ] ;        # Avoid recursion\n",
    "    then\n",
    "    if [[ $i -eq 0 ]] ; then\n",
    "       head -1  $filename >   $OutFileName         # Copy header if it is the first file\n",
    "    fi\n",
    "    tail -n +2  $filename >>  $OutFileName         # Append from the 2nd line each file\n",
    "    i=$(( $i + 1 ))                                # Increase the counter\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We unite these three files in another one.\n",
    "\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LikeSheepsAmongWolves.tmp.utils import cols_attr, cols_glove, cols_empath\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Gets mean and median between tweets\n",
    "tweets = pd.read_csv(\"../data/tweets.csv\")\n",
    "tweets.sort_values(by=[\"user_id\", \"tweet_creation\"], ascending=True, inplace=True)\n",
    "tweets[\"time_diff\"] = tweets.groupby(\"user_id\", sort=False).tweet_creation.diff()\n",
    "time_diff_series_mean = tweets.groupby(\"user_id\", sort=False).time_diff.mean()\n",
    "time_diff_series_median = tweets.groupby(\"user_id\", sort=False).time_diff.median()\n",
    "time_diff = time_diff_series_mean.to_frame()\n",
    "time_diff[\"time_diff_median\"] = time_diff_series_median\n",
    "time_diff.to_csv(\"../data/time_diff.csv\")\n",
    "\n",
    "users_attributes = pd.read_csv(\"../data/users_attributes.csv\")\n",
    "users_content = pd.read_csv(\"../data/users_content.csv\")\n",
    "users_content2 = pd.read_csv(\"../data/users_content2.csv\")\n",
    "users_deleted = pd.read_csv(\"../data/deleted_account.csv\")\n",
    "users_time = pd.read_csv(\"../data/time_diff.csv\")\n",
    "\n",
    "df = pd.merge(users_attributes, users_content, on=\"user_id\", how=\"inner\")\n",
    "df = pd.merge(df, users_content2, on=\"user_id\", how=\"inner\")\n",
    "df = pd.merge(df, users_deleted, on=\"user_id\", how=\"inner\")\n",
    "df = pd.merge(df, users_time, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "df.to_csv(\"../data/users_all.csv\", index=False)\n",
    "\n",
    "users_date = pd.read_csv(\"../data/created_at.csv\")\n",
    "created_at = pd.merge(users_attributes, users_date, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "created_at = created_at[[\"user_id\", \"created_at\", \"hate\", \"hate_neigh\", \"normal_neigh\"]]\n",
    "created_at.to_csv(\"../data/users_created_at.csv\", index=False)\n",
    "\n",
    "\n",
    "cols = cols_attr + cols_glove + cols_empath\n",
    "num_cols = len(cols)\n",
    "\n",
    "graph = nx.read_graphml(\"../data/users_hate.graphml\")\n",
    "df = pd.read_csv(\"../data/users_all.csv\", index_col=0)\n",
    "\n",
    "users = list()\n",
    "for user_id in graph.nodes():\n",
    "    if int(user_id) in df.index.values:\n",
    "        tmp = []\n",
    "        for neighbor in graph.neighbors(user_id):\n",
    "            if int(neighbor) in df.index.values:\n",
    "                tmp.append(list(df.loc[int(neighbor)][cols].values))\n",
    "        users.append([user_id] + list(np.average(np.array(tmp), axis=0)))\n",
    "\n",
    "df = pd.DataFrame.from_records(users, columns=[\"user_id\"] + [\"c_\"+v for v in cols])\n",
    "df.to_csv(\"../data/users_neighborhood.csv\", index=False)\n",
    "\n",
    "users_all = pd.read_csv(\"../data/users_all.csv\")\n",
    "users_neighbor = pd.read_csv(\"../data/users_neighborhood.csv\")\n",
    "df = pd.merge(users_all, users_neighbor, on=\"user_id\", how=\"inner\")\n",
    "df.to_csv(\"../data/users_all_neigh.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abusive_users_osn",
   "language": "python",
   "name": "abusive_users_osn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
