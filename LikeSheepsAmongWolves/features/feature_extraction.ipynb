{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Annotations to Features\n",
    "\n",
    "This Python notebook describes the process of the three files:\n",
    "\n",
    "- `users_infected_diffusion.graphml` with the users their attributes, and the diffusion;\n",
    "\n",
    "- `tweets.csv` with the tweets and their respective users;\n",
    "\n",
    "- `users_to_annotate.csv` a csv file with the 5071 users to be annotated.\n",
    "\n",
    "- `annotated.csv` a csv file with the results in the annotation.\n",
    "\n",
    "- `created_at.csv` a csv file with the creation date for the annotated users. This was collected after the main data collection, due to a bug in the data collection script (which has been fixed).\n",
    "\n",
    "Into the following files:\n",
    "\n",
    "- `users_all_neigh.csv` a csv file with the features extracted for the $100000$ users.\n",
    "\n",
    "- `users_all_neigh_anon.csv` an anonymous version of the previous file.\n",
    "\n",
    "- A set of files to be used by GraphSage:\n",
    "\n",
    "    - `sw-G.json` -- A networkx-specified json file describing the input graph. Nodes have 'val' and 'test' attributes specifying if they are a part of the validation and test sets, respectively.\n",
    "    - `sw-id_map.json` -- A json-stored dictionary mapping the graph node ids to consecutive integers.\n",
    "    - `sw-class_map.json` -- A json-stored dictionary mapping the graph node ids to classes.\n",
    "    - `sw-feats.npy` --- A numpy-stored array of node features; ordering given by id_map.json. Can be omitted and only identity features will be used.\n",
    "    \n",
    "## Profile-based attributes\n",
    "    \n",
    "We begin extracting the median and average time between tweets for each user using the `tweets.csv` file:\n",
    "\n",
    "`tweets.csv` $\\rightarrow$ `time_diff.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv(\"../data/tweets.csv\")\n",
    "tweets.sort_values(by=[\"user_id\", \"tweet_creation\"], ascending=True, inplace=True)\n",
    "tweets[\"time_diff\"] = tweets.groupby(\"user_id\", sort=False).tweet_creation.diff()\n",
    "time_diff_series_mean = tweets.groupby(\"user_id\", sort=False).time_diff.mean()\n",
    "time_diff_series_median = tweets.groupby(\"user_id\", sort=False).time_diff.median()\n",
    "time_diff = time_diff_series_mean.to_frame()\n",
    "time_diff[\"time_diff_median\"] = time_diff_series_median\n",
    "time_diff.to_csv(\"../data/time_diff.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use this time difference, the diffusion graph, and the annotations. We link these values, and calculate centrality measures for the graph, such as betweenness, eigenvector, in degree and out degree.\n",
    "\n",
    "We also set a flag for the **neighbors** of the users who are hateful or normal.\n",
    "\n",
    "`time_diff.csv` `users_infected_diffusion.graphml` `annotated.csv` $\\rightarrow$ `users_hate.graphml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Read annotated users\n",
    "\n",
    "f = open(\"../data/annotated.csv\", \"r\")\n",
    "csv_writer = csv.DictReader(f)\n",
    "\n",
    "set_users = dict()\n",
    "\n",
    "for line in csv_writer:\n",
    "    if line[\"hate\"] == '1':\n",
    "        set_users[line[\"user_id\"]] = 1\n",
    "    elif line[\"hate\"] == \"0\":\n",
    "        set_users[line[\"user_id\"]] = 0\n",
    "f.close()\n",
    "\n",
    "# Read intervals between tweets\n",
    "\n",
    "f = open(\"../data/time_diff.csv\", \"r\")\n",
    "csv_writer = csv.DictReader(f)\n",
    "\n",
    "users_interval_median = dict()\n",
    "users_interval_average = dict()\n",
    "\n",
    "for line in csv_writer:\n",
    "    users_interval_median[line[\"user_id\"]] = line[\"time_diff_median\"]\n",
    "    users_interval_average[line[\"user_id\"]] = line[\"time_diff\"]\n",
    "\n",
    "# Set hate attributes\n",
    "\n",
    "nx_graph = nx.read_graphml(\"../data/users_infected_diffusion.graphml\")\n",
    "nx.set_node_attributes(nx_graph, name=\"hate\", values=-1)\n",
    "nx.set_node_attributes(nx_graph, name=\"hate\", values=set_users)\n",
    "\n",
    "# Set hateful and normal neighbors attribute\n",
    "\n",
    "nodes = nx_graph.nodes(data='hate')\n",
    "\n",
    "hateful_neighbors = dict()\n",
    "normal_neighbors = dict()\n",
    "\n",
    "for i in nodes:\n",
    "    if i[1] == 1:  # hateful node\n",
    "        for j in nx_graph.neighbors(i[0]):\n",
    "            hateful_neighbors[j] = True\n",
    "    if i[1] == 0:\n",
    "        for j in nx_graph.neighbors(i[0]):\n",
    "            normal_neighbors[j] = True\n",
    "\n",
    "nx.set_node_attributes(nx_graph, name=\"hateful_neighbors\", values=False)\n",
    "nx.set_node_attributes(nx_graph, name=\"hateful_neighbors\", values=hateful_neighbors)\n",
    "nx.set_node_attributes(nx_graph, name=\"normal_neighbors\", values=False)\n",
    "nx.set_node_attributes(nx_graph, name=\"normal_neighbors\", values=normal_neighbors)\n",
    "\n",
    "# Set median and average interval attributes\n",
    "\n",
    "nx.set_node_attributes(nx_graph, name=\"median_interval\", values=users_interval_median)\n",
    "nx.set_node_attributes(nx_graph, name=\"average_interval\", values=users_interval_average)\n",
    "\n",
    "# Set node network-based attributes, such as betweenness and eigenvector\n",
    "vt = time.time()\n",
    "betweenness = nx.betweenness_centrality(nx_graph, k=16258, normalized=False)\n",
    "eigenvector = nx.eigenvector_centrality(nx_graph)\n",
    "in_degree = nx.in_degree_centrality(nx_graph)\n",
    "out_degree = nx.out_degree_centrality(nx_graph)\n",
    "\n",
    "nx.set_node_attributes(nx_graph, name=\"betweenness\", values=betweenness)\n",
    "nx.set_node_attributes(nx_graph, name=\"eigenvector\", values=eigenvector)\n",
    "nx.set_node_attributes(nx_graph, name=\"in_degree\", values=in_degree)\n",
    "nx.set_node_attributes(nx_graph, name=\"out_degree\", values=out_degree)\n",
    "\n",
    "nx.write_graphml(nx_graph, \"../data/users_hate.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a csv file with users and these attributes:\n",
    "\n",
    "    user_id            - unique identifier of a user \n",
    "    hate               - hateful|normal|other\n",
    "    hate_neigh         - True|False\n",
    "    normal_neigh       - True|False\n",
    "    statuses_count     - number of statuses\n",
    "    followers_count    - number of followers\n",
    "    followees_count    - number of followees\n",
    "    favorites_count    - number of favorites\n",
    "    listed_count       - number of listed\n",
    "    median_int         - median interval between tweets\n",
    "    average_int        - average interval between tweets\n",
    "    betweenness        - centrality measure\n",
    "    eigenvector        - centrality measure\n",
    "    in_degree          - centrality measure\n",
    "    out_degree         - centrality measure\n",
    "    \n",
    "`users_hate.graphml` $\\rightarrow$ `users_attributes.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "nx_graph = nx.read_graphml(\"../data/users_hate.graphml\")\n",
    "\n",
    "hate = nx.get_node_attributes(nx_graph, \"hate\")\n",
    "hate_n = nx.get_node_attributes(nx_graph, \"hateful_neighbors\")\n",
    "normal_n = nx.get_node_attributes(nx_graph, \"normal_neighbors\")\n",
    "betweenness = nx.get_node_attributes(nx_graph, \"betweenness\")\n",
    "eigenvector = nx.get_node_attributes(nx_graph, \"eigenvector\")\n",
    "in_degree = nx.get_node_attributes(nx_graph, \"in_degree\")\n",
    "out_degree = nx.get_node_attributes(nx_graph, \"out_degree\")\n",
    "statuses_count = nx.get_node_attributes(nx_graph, \"statuses_count\")\n",
    "followers_count = nx.get_node_attributes(nx_graph, \"followers_count\")\n",
    "followees_count = nx.get_node_attributes(nx_graph, \"followees_count\")\n",
    "favorites_count = nx.get_node_attributes(nx_graph, \"favorites_count\")\n",
    "listed_count = nx.get_node_attributes(nx_graph, \"listed_count\")\n",
    "median_interval = nx.get_node_attributes(nx_graph, \"median_interval\")\n",
    "average_interval = nx.get_node_attributes(nx_graph, \"average_interval\")\n",
    "\n",
    "users = []\n",
    "\n",
    "for user_id in hate.keys():\n",
    "    hateful = \"other\"\n",
    "\n",
    "    if hate[user_id] == 1:\n",
    "        hateful = \"hateful\"\n",
    "\n",
    "    elif hate[user_id] == 0:\n",
    "        hateful = \"normal\"\n",
    "\n",
    "    median_int = None if user_id not in median_interval else median_interval[user_id]\n",
    "\n",
    "    average_int = None if user_id not in average_interval else average_interval[user_id]\n",
    "\n",
    "    users.append((user_id, hateful, hate_n[user_id], normal_n[user_id],  # General Stuff\n",
    "                  statuses_count[user_id], followers_count[user_id], followees_count[user_id],\n",
    "                  favorites_count[user_id], listed_count[user_id], median_int,  average_int,  # Numeric attributes\n",
    "                  betweenness[user_id], eigenvector[user_id],  # Network Attributes\n",
    "                  in_degree[user_id], out_degree[user_id]))\n",
    "\n",
    "columns = [\"user_id\", \"hate\", \"hate_neigh\", \"normal_neigh\", \"statuses_count\", \"followers_count\", \"followees_count\",\n",
    "           \"favorites_count\", \"listed_count\", \"median_int\", \"average_int\",\n",
    "           \"betweenness\", \"eigenvector\", \"in_degree\", \"out_degree\"]\n",
    "\n",
    "df = pd.DataFrame.from_records(users, columns=columns)\n",
    "df.to_csv(\"../data/users_attributes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use link `created_at.csv` with the annotations.\n",
    "\n",
    "`created_at.csv` `users_attributes.csv` $\\rightarrow$ `created_at_hate.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This is in case created_at_hate.csv is not generated\n",
    "df1 = pd.read_csv(\"./created_at.csv\")\n",
    "df2 = pd.read_csv(\"./users_attributes.csv\")\n",
    "hateful = dict()\n",
    "\n",
    "for row in df2.iterrows():\n",
    "    if row[1][1] == \"hateful\":\n",
    "        hateful[row[1][0]] = \"hateful\"\n",
    "    if row[1][1] == \"normal\":\n",
    "        hateful[row[1][0]] = \"normal\"\n",
    "\n",
    "to_append = list()\n",
    "for row in df1.iterrows():\n",
    "    if row[1][0] in hateful:\n",
    "        to_append.append(hateful[row[1][0]])\n",
    "    else:\n",
    "        to_append.append(\"None\")\n",
    "\n",
    "df1['hate'] = pd.Series(to_append, index=df1.index)\n",
    "\n",
    "df1.to_csv(\"created_at_hate.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Content-based attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "\n",
    "hashtags = re.compile(\"#(\\w+)\")\n",
    "regex_mentions = re.compile(\"@(\\w+)\")\n",
    "urls = re.compile(\"http(s)?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "regex_bad_words = re.compile(\"(\" + \"|\".join(pd.read_csv(\"../data/bad_words.txt\")[\"words\"].values) + \")\")\n",
    "\n",
    "\n",
    "def mentions(tweets):\n",
    "    ments = []\n",
    "    for tweet in tweets.values:\n",
    "        ments += regex_mentions.findall(tweet)\n",
    "\n",
    "    return len(ments)\n",
    "\n",
    "def bad_words(tweets):\n",
    "    baddies = []\n",
    "    for tweet in tweets.values:\n",
    "        baddies += regex_bad_words.findall(tweet)\n",
    "\n",
    "    return len(baddies)\n",
    "\n",
    "\n",
    "def urls_all(tweets):\n",
    "    urlss = []\n",
    "    for tweet in tweets.values:\n",
    "        urlss += urls.findall(tweet)\n",
    "\n",
    "    return len(urlss)\n",
    "\n",
    "\n",
    "def hashtags_all(tweets):\n",
    "    hts = []\n",
    "    for tweet in tweets.values:\n",
    "        hts += hashtags.findall(tweet)\n",
    "\n",
    "    return len(hts), \" \".join(hts)\n",
    "\n",
    "\n",
    "def get_values(tweets):\n",
    "    c = 0\n",
    "    for tweet in tweets.values:\n",
    "        if tweet != \"\":\n",
    "            c += 1\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "def tweet_size(tweets):\n",
    "    c = 0\n",
    "    for tweet in tweets.values:\n",
    "        c += len(tweet)\n",
    "    return c/len(tweets)\n",
    "\n",
    "\n",
    "def processing(vals, columns, iterv):\n",
    "    users = pd.DataFrame(vals)\n",
    "    users = users[columns]\n",
    "\n",
    "    print(\"{0}-------------\".format(iterv))\n",
    "\n",
    "    # HASHTAGS\n",
    "\n",
    "    users[\"any_text\"] = users[\"tweet_text\"] + users[\"rt_text\"] + users[\"qt_text\"]\n",
    "    users_hashtags = users.groupby([\"user_id\"])[\"any_text\"].apply(lambda x: hashtags_all(x))\n",
    "    hashtags_cols = np.array(list(users_hashtags.values))\n",
    "    df_hashtags = pd.DataFrame(hashtags_cols, columns=[\"number hashtags\", \"hashtags\"], index=users_hashtags.index)\n",
    "    df_hashtags.index.names = ['user_id']\n",
    "\n",
    "    # TWEETS NUMBER\n",
    "\n",
    "    df_tweet_number = users.groupby([\"user_id\"])[\"tweet_text\"].apply(lambda x: get_values(x)).reset_index()\n",
    "    df_tweet_number.set_index(\"user_id\", inplace=True)\n",
    "    df_tweet_number.columns = [\"tweet number\"]\n",
    "\n",
    "    df_retweet_number = users.groupby([\"user_id\"])[\"rt_text\"].apply(lambda x: get_values(x)).reset_index()\n",
    "    df_retweet_number.set_index(\"user_id\", inplace=True)\n",
    "    df_retweet_number.columns = [\"retweet number\"]\n",
    "\n",
    "    df_quote_number = users.groupby([\"user_id\"])[\"qt_text\"].apply(lambda x: get_values(x)).reset_index()\n",
    "    df_quote_number.set_index(\"user_id\", inplace=True)\n",
    "    df_quote_number.columns = [\"quote number\"]\n",
    "\n",
    "    df_tweet_length = users.groupby([\"user_id\"])[\"any_text\"].apply(lambda x: tweet_size(x)).reset_index()\n",
    "    df_tweet_length.set_index(\"user_id\", inplace=True)\n",
    "    df_tweet_length.columns = [\"status length\"]\n",
    "\n",
    "    df_urls = users.groupby([\"user_id\"])[\"any_text\"].apply(lambda x: urls_all(x)).reset_index()\n",
    "    df_urls.set_index(\"user_id\", inplace=True)\n",
    "    df_urls.columns = [\"number urls\"]\n",
    "\n",
    "    df_baddies = users.groupby([\"user_id\"])[\"any_text\"].apply(lambda x: bad_words(x)).reset_index()\n",
    "    df_baddies.set_index(\"user_id\", inplace=True)\n",
    "    df_baddies.columns = [\"baddies\"]\n",
    "\n",
    "    df_mentions = users.groupby([\"user_id\"])[\"any_text\"].apply(lambda x: mentions(x)).reset_index()\n",
    "    df_mentions.set_index(\"user_id\", inplace=True)\n",
    "    df_mentions.columns = [\"mentions\"]\n",
    "\n",
    "    df = pd.DataFrame(pd.concat([df_hashtags, df_tweet_number, df_retweet_number, df_quote_number,\n",
    "                                 df_tweet_length, df_urls, df_baddies, df_mentions], axis=1))\n",
    "    df.to_csv(\"../data/tmp2/users_content_{0}.csv\".format(iterv))\n",
    "    print(\"-------------{0}\".format(iterv))\n",
    "\n",
    "\n",
    "f = open(\"../data/tweets.csv\", \"r\")\n",
    "\n",
    "cols = [\"user_id\", \"screen_name\", \"tweet_id\", \"tweet_text\", \"tweet_creation\", \"tweet_fav\", \"tweet_rt\", \"rp_flag\",\n",
    "        \"rp_status\", \"rp_user\", \"qt_flag\", \"qt_user_id\", \"qt_status_id\", \"qt_text\", \"qt_creation\", \"qt_fav\",\n",
    "        \"qt_rt\", \"rt_flag\", \"rt_user_id\", \"rt_status_id\", \"rt_text\", \"rt_creation\", \"rt_fav\", \"rt_rt\"]\n",
    "\n",
    "csv_dict_reader = csv.DictReader(f)\n",
    "\n",
    "acc_vals = []\n",
    "\n",
    "iter_vals, count, count_max, last_u, v = 1, 0, 50000, None, []\n",
    "for line in csv_dict_reader:\n",
    "    if last_u is not None and last_u != line[\"user_id\"]:\n",
    "        acc_vals.append((v, cols, iter_vals))\n",
    "\n",
    "        count, last_u, v = 0, None, []\n",
    "        iter_vals += 1\n",
    "\n",
    "    if len(acc_vals) == 2:\n",
    "        s = time.time()\n",
    "        processes = []\n",
    "        for i in acc_vals:\n",
    "            p = Process(target=processing, args=(i[0], i[1], i[2]))\n",
    "            processes.append(p)\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        print(time.time() - s)\n",
    "        acc_vals = []\n",
    "\n",
    "    v.append(line)\n",
    "    count += 1\n",
    "    if count >= count_max:\n",
    "        last_u = line[\"user_id\"]\n",
    "\n",
    "s = time.time()\n",
    "processes = []\n",
    "for i in acc_vals:\n",
    "    p = Process(target=processing, args=(i[0], i[1], i[2]))\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.start()\n",
    "for p in processes:\n",
    "    p.join()\n",
    "print(time.time() - s)\n",
    "acc_vals = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing import Process\n",
    "from empath import Empath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textblob\n",
    "import spacy\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "prog = re.compile(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z' \\t])|(\\w+:\\/\\/\\S+)\")\n",
    "prog2 = re.compile(\" +\")\n",
    "lexicon = Empath()\n",
    "empath_cols = [\"{0}_empath\".format(v) for v in lexicon.cats.keys()]\n",
    "glove_cols = [\"{0}_glove\".format(v) for v in range(300)]\n",
    "\n",
    "\n",
    "def lemmatization(x, nlp):\n",
    "    tweets = \" \".join(list(x.values))\n",
    "    letters_only = prog.sub(\" \", tweets)\n",
    "    lemmatized = []\n",
    "    for token1 in nlp(letters_only):\n",
    "        if token1.lemma_ != \"-PRON-\" and token1 not in stopWords:\n",
    "            lemmatized.append(token1.lemma_)\n",
    "        else:\n",
    "            lemmatized.append(token1.text)\n",
    "    final = prog2.sub(\" \", \" \".join(lemmatized))\n",
    "    return final\n",
    "\n",
    "\n",
    "def empath_analysis(x):\n",
    "    val = lexicon.analyze(x, normalize=True)\n",
    "    if val is None:\n",
    "        return lexicon.analyze(x)\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "\n",
    "def processing(vals, columns, iterv):\n",
    "    users = pd.DataFrame(vals)\n",
    "    users = users[columns]\n",
    "\n",
    "    print(\"{0}-------------\".format(iterv))\n",
    "\n",
    "    # PRE-PROCESSING\n",
    "\n",
    "    users[\"any_text\"] = users[\"tweet_text\"] + users[\"rt_text\"] + users[\"qt_text\"]\n",
    "    users_text = users.groupby([\"user_id\"])[\"any_text\"].apply(lambda x: lemmatization(x, nlp)).reset_index()\n",
    "    print(\"{0}-------------PRE-PROCESSING\".format(iterv))\n",
    "\n",
    "    # GLOVE ANALYSIS\n",
    "\n",
    "    glove_arr = np.array(list(users_text[\"any_text\"].apply(lambda x: list(nlp(x).vector)).values))\n",
    "    df_glove = pd.DataFrame(glove_arr, columns=glove_cols, index=users_text.user_id.values)\n",
    "    print(\"{0}-------------GLOVE\".format(iterv))\n",
    "\n",
    "    # SENTIMENT ANALYSIS\n",
    "\n",
    "    sentiment_arr = np.array(list(users_text[\"any_text\"].apply(lambda x: textblob.TextBlob(str(x)).sentiment).values))\n",
    "    sentiment_cols = [\"sentiment\", \"subjectivity\"]\n",
    "    df_sentiment = pd.DataFrame(sentiment_arr, columns=sentiment_cols, index=users_text.user_id.values)\n",
    "    print(\"{0}-------------SENTIMENT\".format(iterv))\n",
    "\n",
    "    # EMPATH ANALYSIS\n",
    "\n",
    "    lexicon_arr = np.array(list(users_text[\"any_text\"].apply(lambda x: empath_analysis(x)).values))\n",
    "    df_empath = pd.DataFrame.from_records(index=users_text.user_id.values, data=lexicon_arr)\n",
    "    df_empath.columns = empath_cols\n",
    "    print(\"{0}-------------EMPATH\".format(iterv))\n",
    "\n",
    "    # MERGE TO SINGLE\n",
    "\n",
    "    df = pd.DataFrame(pd.concat([df_empath, df_sentiment, df_glove], axis=1))\n",
    "    df.set_index(\"user_id\", inplace=True)\n",
    "    df.to_csv(\"../data/tmp/users_content_{0}.csv\".format(iterv))\n",
    "    print(\"-------------{0}\".format(iterv))\n",
    "\n",
    "\n",
    "f = open(\"../data/tweets.csv\", \"r\")\n",
    "\n",
    "cols = [\"user_id\", \"screen_name\", \"tweet_id\", \"tweet_text\", \"tweet_creation\", \"tweet_fav\", \"tweet_rt\", \"rp_flag\",\n",
    "        \"rp_status\", \"rp_user\", \"qt_flag\", \"qt_user_id\", \"qt_status_id\", \"qt_text\", \"qt_creation\", \"qt_fav\",\n",
    "        \"qt_rt\", \"rt_flag\", \"rt_user_id\", \"rt_status_id\", \"rt_text\", \"rt_creation\", \"rt_fav\", \"rt_rt\"]\n",
    "\n",
    "csv_dict_reader = csv.DictReader(f)\n",
    "\n",
    "acc_vals = []\n",
    "\n",
    "iter_vals, count, count_max, last_u, v = 1, 0, 50000, None, []\n",
    "for line in csv_dict_reader:\n",
    "    if last_u is not None and last_u != line[\"user_id\"]:\n",
    "        # s = time.time()\n",
    "        # processing(v, cols, iter_vals)\n",
    "        # print(time.time() - s)\n",
    "        acc_vals.append((v, cols, iter_vals))\n",
    "\n",
    "        count, last_u, v = 0, None, []\n",
    "        iter_vals += 1\n",
    "\n",
    "    if len(acc_vals) == 2:\n",
    "        s = time.time()\n",
    "        processes = []\n",
    "        for i in acc_vals:\n",
    "            p = Process(target=processing, args=(i[0], i[1], i[2]))\n",
    "            processes.append(p)\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        print(time.time() - s)\n",
    "        acc_vals = []\n",
    "\n",
    "    v.append(line)\n",
    "    count += 1\n",
    "    if count >= count_max:\n",
    "        last_u = line[\"user_id\"]\n",
    "\n",
    "# s = time.time()\n",
    "# processing(v, cols, iter_vals)\n",
    "# print(time.time() - s)\n",
    "\n",
    "s = time.time()\n",
    "processes = []\n",
    "for i in acc_vals:\n",
    "    p = Process(target=processing, args=(i[0], i[1], i[2]))\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.start()\n",
    "for p in processes:\n",
    "    p.join()\n",
    "print(time.time() - s)\n",
    "acc_vals = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OutFileName=\"../data/users_content.csv\"            # Fix the output name\n",
    "i=0                                                # Reset a counter\n",
    "for filename in ../data/tmp/*.csv; do\n",
    "    if [ \"$filename\"  != \"$OutFileName\" ] ;        # Avoid recursion\n",
    "    then\n",
    "    if [[ $i -eq 0 ]] ; then\n",
    "       head -1  $filename >   $OutFileName         # Copy header if it is the first file\n",
    "    fi\n",
    "    tail -n +2  $filename >>  $OutFileName         # Append from the 2nd line each file\n",
    "    i=$(( $i + 1 ))                                # Increase the counter\n",
    "    fi\n",
    "done\n",
    "OutFileName=\"../data/users_content2.csv\"           # Fix the output name\n",
    "i=0                                                # Reset a counter\n",
    "for filename in ../data/tmp2/*.csv; do\n",
    "    if [ \"$filename\"  != \"$OutFileName\" ] ;        # Avoid recursion\n",
    "    then\n",
    "    if [[ $i -eq 0 ]] ; then\n",
    "       head -1  $filename >   $OutFileName         # Copy header if it is the first file\n",
    "    fi\n",
    "    tail -n +2  $filename >>  $OutFileName         # Append from the 2nd line each file\n",
    "    i=$(( $i + 1 ))                                # Increase the counter\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "users_attributes = pd.read_csv(\"../data/users_attributes.csv\")\n",
    "users_content = pd.read_csv(\"../data/users_content.csv\")\n",
    "users_content2 = pd.read_csv(\"../data/users_content2.csv\")\n",
    "\n",
    "df = pd.merge(users_attributes, users_content, on=\"user_id\", how=\"inner\")\n",
    "df = pd.merge(df, users_content2, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "df.to_csv(\"../data/users_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abusive_users_osn",
   "language": "python",
   "name": "abusive_users_osn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
